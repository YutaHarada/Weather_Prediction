{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練用データセットを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/kochi.csv\")\n",
    "df_tokyo_rain  = pd.read_csv(\"./data/tokyo.csv\", usecols=[\"降水量\"])\n",
    "df[\"target\"] = df_tokyo_rain[\"降水量\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "df[\"target\"] = df[\"target\"].shift(-1)\n",
    "df[\"降水量\"] = df[\"降水量\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "df = df.query(\"気圧 != 0.0\").copy()\n",
    "df = df.query(\"最高気温 != 0\").copy()\n",
    "df = df.query(\"最小湿度 != 0\").copy()\n",
    "df = df.query(\"平均湿度 != 0\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 余分な記号や空白を除外する\n",
    "def modify_string(x):\n",
    "    x = re.sub(r'[\\)|\\]|\\s]', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['最大風速（風向）'] = df['最大風速（風向）'].apply(modify_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時間情報を三角関数で変換\n",
    "def trig_encoding(df, col):\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / 12)\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / 12)\n",
    "    # 不要列を削除\n",
    "    df.drop(columns=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trig_encoding(df, \"月\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Vector化\n",
    "def one_hot_encoding(df, col):\n",
    "    # One-Hot-Vectorの取得\n",
    "    data_dummy = pd.get_dummies(df[col], drop_first=True, dtype='int64')\n",
    "    print(\"カテゴリ特徴量\")\n",
    "    print(data_dummy.columns)\n",
    "\n",
    "    # 元データと結合し不要列を削除する\n",
    "    df = pd.concat([df, data_dummy], axis=1)\n",
    "    df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # 全カラムが数値型になっていることを確認する\n",
    "    print(\"各特徴量の方を確認\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_encoding(df, \"最大風速（風向）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要列の削除\n",
    "datasets = df.drop(columns=[\"年\",\"日\"])\n",
    "# target列のNaNを0(2023/1/1の東京は降雨なし)で埋める\n",
    "datasets['target'].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの中身を確認\n",
    "print(datasets.info())\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 連続値のデータのみ標準化を実施する\n",
    "def stdnum(num_features, data):\n",
    "    processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    data_processed = processor.fit_transform(data)\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数と説明変数に分離してホールドアウト検証を実施\n",
    "def split_variable(datasets, test_size, random_state):\n",
    "    explains = datasets.columns[datasets.columns != 'target']\n",
    "    X = datasets[explains]\n",
    "    y = datasets[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存する\n",
    "def save_model(name, mod):\n",
    "       with open(f\"./models/pred_rain_{name}.model\", \"wb\") as f:\n",
    "              pickle.dump(mod, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_variable(datasets, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['気圧', '平均気温', '最高気温', '最低気温', '平均湿度', '最小湿度', '平均風速', '最大風速（風速）', '日照時間', '月_cos', '月_sin']\n",
    "X_train_processed = stdnum(num_features,X_train)\n",
    "X_test_processed = stdnum(num_features,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = LogisticRegression(\n",
    "        C=trial.suggest_float('C', 1e-5, 1e+2, log=True),\n",
    "        random_state=trial.suggest_int('random_state',0,200, log=False),\n",
    "        max_iter=1500       \n",
    "    )\n",
    "    \n",
    "    result = cross_validate(estimator=model, X=X_train_processed, y=y_train, cv=10, scoring='accuracy')\n",
    "    val_accuracy = result['test_score'].mean()\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 最適化を実行\n",
    "if __name__ == \"__main__\":\n",
    "    # 実行ログを非表示\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    # studyの作成\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 最適化の実施\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # 最良のトライアルの確認\n",
    "    print(\" Best trial : \")\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(f\" Value : {best_trial.value} \")\n",
    "\n",
    "    print(\" Params : \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\" {key} : {value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = LogisticRegression(C=best_trial.params['C'], random_state=best_trial.params['random_state'])\n",
    "best_lr.fit(X_train_processed, y_train)\n",
    "pred_test = best_lr.predict(X_test_processed)\n",
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_model(\"lr\", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多層ニューラルネットワークモデル作成用の関数\n",
    "def create_model(trial):\n",
    "    \n",
    "    # AdamWの学習率の探索範囲を設定\n",
    "    optimeizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "    )\n",
    "\n",
    "    # 多層FNNの構築\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=28, name=f'hidden_1', activation='relu')\n",
    "    ])\n",
    "    for i in range(2,5):\n",
    "        model.add(tf.keras.layers.Dense(units=100, name=f'hidden_{i}', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=1, name='output', activation='sigmoid'))\n",
    "    model.build(input_shape=(None, 28))\n",
    "\n",
    "    # 最適化手法、損失関数、評価指標の設定\n",
    "    model.compile(optimizer=optimeizer,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの指定（最適化対象外）\n",
    "tf.random.set_seed(1)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "validation_split = 0.2\n",
    "step_per_epoch = np.ceil(X_train_processed.shape[0] * (1-validation_split) / BATCH_SIZE)    \n",
    "validation_steps = np.ceil(X_train_processed.shape[0] * validation_split / BATCH_SIZE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optunaにより最小化したい目的関数を準備\n",
    "def objective(trial):\n",
    "     \n",
    "    # モデルの生成\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # 過学習対策に EarlyStopping コールバックを設定。val_lossの値が３エポックに渡って改善されなかった場合に学習を中止する\n",
    "    # 効率化のため TFKerasPruning コールバックを設定。精度が出る見込みが薄いハイパーパラメータの組み合わせについては早々に切り捨てる\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "        optuna.integration.TFKerasPruningCallback(trial, monitor='val_accuracy'),\n",
    "    ]\n",
    "    \n",
    "    # モデルの訓練\n",
    "    history = model.fit(X_train_processed, y_train,\n",
    "                        validation_split=validation_split,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=EPOCHS, \n",
    "                        steps_per_epoch=step_per_epoch, \n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # 最後の val_accuracy を出力\n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化を実行\n",
    "if __name__ == \"__main__\":\n",
    "    # studyの作成。'枝刈り'の方法としてはMedianPrunerを設定\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=2)\n",
    "        )\n",
    "    \n",
    "    # 最適化の実施\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # 途中で枝刈りされたトライアルの数と、最後まで完了したトライアルの数を取得\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n",
    "    \n",
    "    # トライアル回数の確認\n",
    "    print(\"Study statistics: \")\n",
    "    print(f\" Number of finished trials : {len(study.trials)} \")\n",
    "    print(f\" Number of pruned trials : {len(pruned_trials)} \")\n",
    "    print(f\" Number of complete trials : {len(complete_trials)} \")\n",
    "\n",
    "    # 最良のトライアルの確認\n",
    "    print(\" Best trial : \")\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(f\" Value : {best_trial.value} \")\n",
    "\n",
    "    print(\" Params : \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\" {key} : {value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も正解率の高かったハイパーパラメータの組み合わせを用いてモデルを生成し訓練を実施する\n",
    "best_nn = create_model(best_trial)\n",
    "\n",
    "# 過学習対策で EarlyStopping を設定\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "]\n",
    "\n",
    "# モデルの訓練\n",
    "history = best_nn.fit(X_train_processed, y_train,\n",
    "                         validation_split=validation_split,\n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         epochs=EPOCHS, \n",
    "                         steps_per_epoch=step_per_epoch, \n",
    "                         validation_steps=validation_steps,\n",
    "                         callbacks=callbacks,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを用いてモデルの汎化性能を評価する\n",
    "test_eval = best_nn.evaluate(X_test_processed, y_test)\n",
    "print('Test Acc :', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_model(\"nn\", best_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "     \n",
    "     n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "     max_depth = trial.suggest_int('max_depth', 1, 32, log=True)\n",
    "     max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "     criterion = trial.suggest_categorical('criterion', [\"gini\", \"entropy\"])\n",
    "     random_state = random_state=trial.suggest_int('random_state',0,200, log=False)\n",
    "\n",
    "     clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  criterion=criterion,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "     result = cross_validate(estimator=clf, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "     val_accuracy = result['test_score'].mean()\n",
    "    \n",
    "     return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 最適化を実行\n",
    "if __name__ == \"__main__\":\n",
    "    # 実行ログを非表示\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    # studyの作成\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 最適化の実施\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # 最良のトライアルの確認\n",
    "    print(\" Best trial : \")\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(f\" Value : {best_trial.value} \")\n",
    "\n",
    "    print(\" Params : \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\" {key} : {value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = RandomForestClassifier(n_estimators=best_trial.params[\"n_estimators\"],\n",
    "                            max_depth=best_trial.params[\"max_depth\"],\n",
    "                            max_features=best_trial.params[\"max_features\"],\n",
    "                            criterion=best_trial.params[\"criterion\"],\n",
    "                            random_state=best_trial.params[\"random_state\"])\n",
    "best_rfc.fit(X_train, y_train)\n",
    "pred_test = best_rfc.predict(X_test)\n",
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_model(\"rfc\", best_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
